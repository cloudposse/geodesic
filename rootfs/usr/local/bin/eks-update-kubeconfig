#!/bin/bash

## Right now this has the same limitation that `aws eks update-kubeconfig` has,
## which is that it will only create a config for a cluster in the same account
## as your profile.
##
## TODO: Enhance to be able to create a config for an accessible cluster in another account
##

# We expect AWS_DEFAULT_SHORT_REGION was set in Dockerfile, and if not there,
# then in aws.sh, but just in case...
AWS_DEFAULT_SHORT_REGION=${AWS_DEFAULT_SHORT_REGION:-$(aws-region --${AWS_REGION_ABBREVIATION_TYPE} ${AWS_DEFAULT_REGION:-us-west-2})}

function usage() {
	cat >&2 <<'EOF'
Usage:
  eks-update-kubeconfig <cluster-short-name> [role-short-name]
  eks-update-kubeconfig set-kubeconfig <cluster-short-name> [role-short-name]
  eks-update-kubeconfig off

  With <cluster-short-name> updates the kubecfg file for the cluster with that short name (e.g. "corp")
  using the role provided or the current role (from environment variable ASSUME_ROLE) if none specified.

  With "set-kubeconfig" outputs the fully-qualified name of the file generated by this command.

  With "off", deletes the currently active kubecfg file.

  NOTE: This tool assumes you are using Cloud Posse's standard naming conventions:
  * Cluster name "uw2-corp" expands to "${NAMESPACE}-uw2-corp-eks-cluster"
  * Role name "admin" expands to "${NAMESPACE}-gbl-corp-admin"

EOF
}

function profile_name() {
	local profile_arg="${2:-${ASSUME_ROLE}}"
	printf "%s" "${NAMESPACE}-gbl-${1#*-}-${profile_arg##*-}"
}

function file_name() {
	KUBECONFIG_DIR=$(dirname ${KUBECONFIG:-/dev/shm/kubecfg})
	EKS_KUBECONFIG_PATTERN="${EKS_KUBECONFIG_PATTERN:-${KUBECONFIG_DIR}/kubecfg.%s-%s}"
	local profile=$(profile_name $1 $2)
	printf "${EKS_KUBECONFIG_PATTERN}" "$1" "${profile##*-}"
}

function short_region() {
  	if [[ $1 =~ ^[a-z]+$ ]]; then
  	  echo ${AWS_DEFAULT_SHORT_REGION}
  	else
  	  printf "%s" "$1" | cut -f1 -d-
  	fi
}

function region() {
  aws-region $(short_region $1) 2>/dev/null || { red could not find region in "$1" >&2; echo none; return 1; }
}

function red() {
	echo "$(tput setaf 1)$*$(tput sgr0)"
}

main() {
	if (($# == 0)); then
		usage
		return 1
	fi
	if [[ $KUBECONFIG =~ ":" ]]; then
		red "$0 requires that KUBECONFIG point to a single file, not a set of directories" >&2
		return 9
	fi
	if [[ $1 == "off" ]]; then
		if [[ -n $KUBECONFIG ]] && [[ -f $KUBECONFIG ]]; then
			rm -f $KUBECONFIG
		fi
		return 0
	fi
	local role
	if [[ $1 == "set-kubeconfig" ]]; then
		role="$3"
	else
		role="$2"
	fi
	if [[ -z $role ]]; then
		role="$ASSUME_ROLE"
		if [[ $role =~ identity$ ]]; then
			role=$(aws sts get-caller-identity --output text --query 'Arn' | cut -d/ -f2)
		fi
	fi
	if [[ -z ${role} ]]; then
		printf "%s: %s\n\n" "$(basename $0)" "$(red "* No role specified and cannot guess role. Giving up.")" >&2
		usage
		return 9
	fi
	if [[ $1 == "set-kubeconfig" ]]; then
		file_name $2 $role
		return 0
	fi

	local kubecfg="$(file_name $1 $role)"
	EKS_CLUSTER_NAME_PATTERN="${EKS_CLUSTER_NAME_PATTERN:-${NAMESPACE:+${NAMESPACE}-}%s-eks-cluster}"
	aws --profile $(profile_name $1 $role) --region $(region $1) eks update-kubeconfig --name=$(printf "$EKS_CLUSTER_NAME_PATTERN" $1) --kubeconfig="$kubecfg"
	chmod 600 "$kubecfg"
}

main "$@"
